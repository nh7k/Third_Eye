# -*- coding: utf-8 -*-
"""saktinet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1swTbfejVDGadmJF1PWVTSO6K8_W3-5Gr
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/   # first you uploaded kaggle.json first
!chmod 600 ~/.kaggle/kaggle.json

!pip install kaggle
import os
import pandas as pd

!kaggle datasets download -d elakiricoder/gender-classification-dataset

# Unzip
!unzip -q gender-classification-dataset.zip

# Load into pandas
import pandas as pd
df = pd.read_csv("gender_classification_v7.csv")
print(df.head())
print(df['gender'].value_counts())

# ----------------------------------------------
# Load image paths and labels using tf.data.Dataset
# ----------------------------------------------
import tensorflow as tf
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split # Import train_test_split
from tqdm import tqdm # Ensure tqdm is imported
from tensorflow.keras import models, layers # Ensure models and layers are imported
import matplotlib.pyplot as plt

image_dir = "UTKFace/crop_part1"
IMG_SIZE = (128, 128)
BATCH_SIZE = 32

# Get list of image paths and corresponding genders
image_paths = []
genders = []

print(f"Collecting image paths and genders from: {image_dir}")
if not os.path.exists(image_dir):
    print(f"Error: Directory '{image_dir}' not found.")
else:
    files_in_dir = os.listdir(image_dir)
    print(f"Found {len(files_in_dir)} files.")
    for file in tqdm(files_in_dir):
        if file.endswith(".jpg"):
            try:
                parts = file.split("_")
                if len(parts) > 1:
                    gender_str = parts[1]
                    try:
                        gender = int(gender_str)
                        if gender in [0, 1]:
                            image_paths.append(os.path.join(image_dir, file))
                            genders.append(gender)
                    except ValueError:
                        # print(f"Warning: Skipping file with non-integer gender part '{gender_str}': {file}")
                        pass # Suppress frequent warnings
                else:
                    # print(f"Warning: Skipping file with unexpected filename format (not enough parts): {file}")
                    pass # Suppress frequent warnings
            except Exception as e:
                print(f"Error processing file {file}: {e}")

print(f"Collected {len(image_paths)} image paths.")

# Split the data into train, validation, and test sets using sklearn
# This is done on paths/labels to avoid loading all images into memory at once
train_paths, tmp_paths, train_genders, tmp_genders = train_test_split(
    image_paths, genders, test_size=0.3, stratify=genders, random_state=42
)
val_paths, test_paths, val_genders, test_genders = train_test_split(
    tmp_paths, tmp_genders, test_size=0.5, stratify=tmp_genders, random_state=42
)

print(f"Train size: {len(train_paths)}, Validation size: {len(val_paths)}, Test size: {len(test_paths)}")


# Function to load and preprocess images for tf.data
def load_and_preprocess_image(image_path, gender):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMG_SIZE)
    img = img / 255.0  # Normalize
    return img, gender

# Create tf.data.Dataset for each split
train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_genders))
val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_genders))
test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_genders))

# Apply preprocessing, shuffle, and batch
train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE) # No shuffle for validation

test_dataset = test_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)


# ----------------------------------------------
# Build CNN model
# ----------------------------------------------
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

# ----------------------------------------------
# Train model
# ----------------------------------------------
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=10 # Reduced epochs for faster execution
)

# ----------------------------------------------
# Evaluate the model
# ----------------------------------------------
print("\nEvaluating the model...")
test_loss, test_acc = model.evaluate(test_dataset)
print(f"\nâœ… Test Accuracy: {test_acc:.3f}")


# Plot training curves
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend()
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()


# ----------------------------------------------
# Predict on specified image
# ----------------------------------------------
image_path_to_predict = "/content/R.jpg"
if os.path.exists(image_path_to_predict):
    print(f"\nPredicting gender for: {image_path_to_predict}")
    img = cv2.imread(image_path_to_predict)
    if img is not None:
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (IMG_SIZE[0], IMG_SIZE[1]))
        img_norm = img_resized / 255.0
        prediction = model.predict(np.expand_dims(img_norm, axis=0))[0][0]
        label = "Male" if prediction > 0.5 else "Female"

        plt.imshow(img_rgb)
        plt.title(f"Predicted Gender: {label}")
        plt.axis('off')
        plt.show()
    else:
        print(f"Error: Could not read image file: {image_path_to_predict}")

else:
    print(f"Error: Image not found at {image_path_to_predict}")

# Save your trained CNN model
model.save("gender_cnn_model.h5")
print("âœ… Model saved as gender_cnn_model.h5")

# ----------------------------------------------
# Load image paths and labels
# ----------------------------------------------
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm

image_dir = "UTKFace/crop_part1"
IMG_SIZE = (128, 128)

image_paths = []
genders = []

print(f"Collecting image paths and genders from: {image_dir}")
if not os.path.exists(image_dir):
    print(f"Error: Directory '{image_dir}' not found.")
else:
    files_in_dir = os.listdir(image_dir)
    print(f"Found {len(files_in_dir)} files.")
    for file in tqdm(files_in_dir):
        if file.endswith(".jpg"):
            try:
                parts = file.split("_")
                if len(parts) > 1:
                    gender_str = parts[1]
                    try:
                        gender = int(gender_str)
                        if gender in [0, 1]:
                            image_paths.append(os.path.join(image_dir, file))
                            genders.append(gender)
                    except ValueError:
                        pass
                else:
                    pass
            except Exception as e:
                print(f"Error processing file {file}: {e}")

print(f"Collected {len(image_paths)} image paths.")

# Split the data into train, validation, and test sets using sklearn
train_paths, tmp_paths, train_genders, tmp_genders = train_test_split(
    image_paths, genders, test_size=0.3, stratify=genders, random_state=42
)
val_paths, test_paths, val_genders, test_genders = train_test_split(
    tmp_paths, tmp_genders, test_size=0.5, stratify=tmp_genders, random_state=42
)

print(f"Train size: {len(train_paths)}, Validation size: {len(val_paths)}, Test size: {len(test_paths)}")

# ----------------------------------------------
# Create tf.data.Dataset using from_generator
# ----------------------------------------------
import tensorflow as tf

# Generator function to yield image-label pairs
def data_generator(image_paths, genders):
    for img_path, gender in zip(image_paths, genders):
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, IMG_SIZE)
            img = img / 255.0  # Normalize
            yield img, gender

# Create datasets using from_generator
train_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(train_paths, train_genders),
    output_types=(tf.float32, tf.int32),
    output_shapes=((IMG_SIZE[0], IMG_SIZE[1], 3), ())
)

val_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(val_paths, val_genders),
    output_types=(tf.float32, tf.int32),
    output_shapes=((IMG_SIZE[0], IMG_SIZE[1], 3), ())
)

test_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(test_paths, test_genders),
    output_types=(tf.float32, tf.int32),
    output_shapes=((IMG_SIZE[0], IMG_SIZE[1], 3), ())
)

# Batch and prefetch
BATCH_SIZE = 32
train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

print("tf.data.Dataset created for train, validation, and test.")
print(f"Train dataset element spec: {train_dataset.element_spec}")
print(f"Validation dataset element spec: {val_dataset.element_spec}")
print(f"Test dataset element spec: {test_dataset.element_spec}")

"""next target

just try
"""

"""
FIXED GENDER PREDICTION CODE
The issue was with model output interpretation and image preprocessing
"""

import cv2
import numpy as np
from tensorflow.keras.models import load_model

# ============================================================================
# LOAD MODEL
# ============================================================================
GENDER_MODEL_PATH = "/content/gender_cnn_model.h5"
gender_model = load_model(GENDER_MODEL_PATH)
print("âœ… Gender CNN model loaded.")

# Get model input shape
model_input_shape = gender_model.input_shape
IMG_SIZE = (model_input_shape[1], model_input_shape[2])  # (height, width)
print(f"ðŸ“ Model expects input size: {IMG_SIZE}")

# Check model output
print(f"ðŸ“Š Model output shape: {gender_model.output_shape}")

# ============================================================================
# FACE DETECTION (Keep as is)
# ============================================================================
FACE_CASCADE = cv2.CascadeClassifier(
    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
)

def get_face_from_person(person_bgr):
    """Extract face from person crop using Haar Cascade"""
    if person_bgr is None or person_bgr.size == 0:
        return None

    gray = cv2.cvtColor(person_bgr, cv2.COLOR_BGR2GRAY)
    faces = FACE_CASCADE.detectMultiScale(gray, 1.2, 3, minSize=(32, 32))

    if len(faces) > 0:
        x, y, w, h = max(faces, key=lambda f: f[2] * f[3])  # largest face
        return person_bgr[y:y+h, x:x+w]

    # Fallback: upper center region (likely contains face)
    H, W = person_bgr.shape[:2]
    y1, y2 = 0, max(32, int(0.45 * H))
    x1, x2 = int(0.2 * W), int(0.8 * W)
    face_region = person_bgr[y1:y2, x1:x2]

    return face_region if face_region.size > 0 else person_bgr


# ============================================================================
# FIXED GENDER PREDICTION
# ============================================================================
def predict_gender(person_bgr):
    """
    FIXED VERSION - Correctly predicts gender from person crop

    Key fixes:
    1. Proper image preprocessing (match training)
    2. Correct interpretation of model output
    3. Handle both binary and multi-class outputs
    """
    if person_bgr is None or person_bgr.size == 0:
        return "Unknown"

    # Step 1: Get face (or full person if face not detected)
    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        face = person_bgr

    # Step 2: Resize to model input size
    img = cv2.resize(face, IMG_SIZE)

    # Step 3: Convert BGR to RGB (CRITICAL!)
    # OpenCV uses BGR, but most models train on RGB
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Step 4: Normalize (match your training preprocessing!)
    img_normalized = img_rgb.astype("float32") / 255.0

    # Step 5: Add batch dimension
    img_batch = np.expand_dims(img_normalized, axis=0)

    # Step 6: Predict
    pred = gender_model.predict(img_batch, verbose=0)

    # Step 7: Interpret output correctly
    # Your model likely outputs [female_prob, male_prob] or single sigmoid

    if pred.shape[-1] == 1:
        # Single sigmoid output
        prob = float(pred[0][0])

        # IMPORTANT: Check what your model was trained with
        # If 0=female, 1=male: use prob > 0.5 for male
        # If 0=male, 1=female: use prob > 0.5 for female

        # Most common: 0=female, 1=male (Keras default with male as positive class)
        gender = "Male" if prob > 0.5 else "Female"
        confidence = prob if gender == "Male" else 1 - prob

    elif pred.shape[-1] == 2:
        # Two outputs: [class_0_prob, class_1_prob]
        female_prob = float(pred[0][0])
        male_prob = float(pred[0][1])

        # Determine gender by highest probability
        if male_prob > female_prob:
            gender = "Male"
            confidence = male_prob
        else:
            gender = "Female"
            confidence = female_prob

    else:
        # Multi-class (shouldn't happen for binary gender)
        gender_idx = int(np.argmax(pred[0]))
        gender = "Male" if gender_idx == 1 else "Female"
        confidence = float(pred[0][gender_idx])

    # Debug: Print predictions occasionally (remove after testing)
    # print(f"Debug: Raw pred={pred[0]}, Gender={gender}, Conf={confidence:.2f}")

    return gender


# ============================================================================
# ALTERNATIVE: Test your model to understand output format
# ============================================================================
def test_model_output():
    """
    Run this ONCE to understand your model's output format
    """
    print("\n" + "="*60)
    print("TESTING MODEL OUTPUT FORMAT")
    print("="*60)

    # Create dummy image
    dummy_img = np.random.rand(1, IMG_SIZE[0], IMG_SIZE[1], 3).astype('float32')

    # Get prediction
    pred = gender_model.predict(dummy_img, verbose=0)

    print(f"\nðŸ“Š Output shape: {pred.shape}")
    print(f"ðŸ“Š Output values: {pred[0]}")
    print(f"ðŸ“Š Output sum: {np.sum(pred[0]):.4f}")

    if pred.shape[-1] == 1:
        print("\nâœ… Model type: Single sigmoid output")
        print("   - Output range: 0-1")
        print("   - Interpretation: >0.5 typically means positive class")
    elif pred.shape[-1] == 2:
        print("\nâœ… Model type: Two-class softmax output")
        print("   - Output range: Both sum to ~1.0")
        print(f"   - Class 0 (likely Female): {pred[0][0]:.4f}")
        print(f"   - Class 1 (likely Male): {pred[0][1]:.4f}")

    print("\n" + "="*60)

# Uncomment to test:
# test_model_output()


# ============================================================================
# ENHANCED VERSION: Test with actual image
# ============================================================================
def test_with_sample_image(image_path):
    """
    Test prediction on a sample image to verify correctness
    Usage: test_with_sample_image("sample_male.jpg")
    """
    img = cv2.imread(image_path)
    if img is None:
        print(f"âŒ Could not load: {image_path}")
        return

    gender = predict_gender(img)
    print(f"\nðŸ“¸ Image: {image_path}")
    print(f"ðŸ‘¤ Predicted: {gender}")

    # Show prediction on image
    cv2.putText(img, f"Predicted: {gender}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow("Prediction Test", img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


# ============================================================================
# QUICK FIX: If predictions are REVERSED
# ============================================================================
def predict_gender_REVERSED(person_bgr):
    """
    Use this if your predictions are consistently backwards
    (detecting males as females and vice versa)
    """
    if person_bgr is None or person_bgr.size == 0:
        return "Unknown"

    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        face = person_bgr

    img = cv2.resize(face, IMG_SIZE)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_normalized = img_rgb.astype("float32") / 255.0
    img_batch = np.expand_dims(img_normalized, axis=0)

    pred = gender_model.predict(img_batch, verbose=0)

    if pred.shape[-1] == 1:
        prob = float(pred[0][0])
        # REVERSED: flip the interpretation
        gender = "Female" if prob > 0.5 else "Male"  # <-- SWAPPED
        confidence = prob if gender == "Female" else 1 - prob

    elif pred.shape[-1] == 2:
        female_prob = float(pred[0][0])
        male_prob = float(pred[0][1])

        # REVERSED: swap the class assignments
        if male_prob > female_prob:
            gender = "Female"  # <-- SWAPPED
            confidence = male_prob
        else:
            gender = "Male"  # <-- SWAPPED
            confidence = female_prob

    return gender


# ============================================================================
# RECOMMENDED: Add confidence threshold
# ============================================================================
def predict_gender_with_confidence(person_bgr, confidence_threshold=0.7):
    """
    Only return gender if confidence is above threshold
    Otherwise return "Unknown"
    """
    if person_bgr is None or person_bgr.size == 0:
        return "Unknown"

    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        face = person_bgr

    img = cv2.resize(face, IMG_SIZE)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_normalized = img_rgb.astype("float32") / 255.0
    img_batch = np.expand_dims(img_normalized, axis=0)

    pred = gender_model.predict(img_batch, verbose=0)

    if pred.shape[-1] == 1:
        prob = float(pred[0][0])
        gender = "Male" if prob > 0.5 else "Female"
        confidence = prob if gender == "Male" else 1 - prob

    elif pred.shape[-1] == 2:
        female_prob = float(pred[0][0])
        male_prob = float(pred[0][1])

        if male_prob > female_prob:
            gender = "Male"
            confidence = male_prob
        else:
            gender = "Female"
            confidence = female_prob

    # Return Unknown if confidence too low
    if confidence < confidence_threshold:
        return "Unknown"

    return gender


# ============================================================================
# DEBUGGING: Print detailed prediction info
# ============================================================================
def predict_gender_DEBUG(person_bgr):
    """
    Use this version to see what's happening
    """
    if person_bgr is None or person_bgr.size == 0:
        print("âš ï¸  Empty person crop")
        return "Unknown"

    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        print("âš ï¸  No face detected, using full crop")
        face = person_bgr

    print(f"ðŸ“ Face crop size: {face.shape}")

    img = cv2.resize(face, IMG_SIZE)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_normalized = img_rgb.astype("float32") / 255.0
    img_batch = np.expand_dims(img_normalized, axis=0)

    print(f"ðŸ“Š Input to model: shape={img_batch.shape}, min={img_batch.min():.3f}, max={img_batch.max():.3f}")

    pred = gender_model.predict(img_batch, verbose=0)
    print(f"ðŸ“Š Model output: {pred[0]}")

    if pred.shape[-1] == 1:
        prob = float(pred[0][0])
        gender = "Male" if prob > 0.5 else "Female"
        confidence = prob if gender == "Male" else 1 - prob
        print(f"ðŸ‘¤ Single output: prob={prob:.4f}, gender={gender}, confidence={confidence:.4f}")

    elif pred.shape[-1] == 2:
        female_prob = float(pred[0][0])
        male_prob = float(pred[0][1])
        gender = "Male" if male_prob > female_prob else "Female"
        confidence = max(female_prob, male_prob)
        print(f"ðŸ‘¤ Two outputs: female={female_prob:.4f}, male={male_prob:.4f}, final={gender}")

    return gender


print("\nâœ… Fixed gender prediction functions loaded!")
print("\nðŸ“ Usage:")
print("   - Use: predict_gender(person_crop)")
print("   - Debug: predict_gender_DEBUG(person_crop)")
print("   - If reversed: predict_gender_REVERSED(person_crop)")
print("   - Test output: test_model_output()")

# Commented out IPython magic to ensure Python compatibility.
# %pip install mediapipe

"""The error `ModuleNotFoundError: No module named 'mediapipe'` means that the `mediapipe` library was not found in the current environment. This typically happens when a required library hasn't been installed. The cell above installs the `mediapipe` library. After installation, the import statement in the code cell should work correctly."""

from google.colab.patches import cv2_imshow

"""For example, here we download and display a PNG image of the Colab logo:"""

!curl -o logo.png https://colab.research.google.com/img/colab_favicon_256px.png
import cv2
img = cv2.imread('logo.png', cv2.IMREAD_UNCHANGED)
cv2_imshow(img)

import tensorflow as tf

model_path = "/content/gender_cnn_model.h5"

try:
    model = tf.keras.models.load_model(model_path)
    print(f"âœ… Model loaded successfully from {model_path}")
    model.summary()
except Exception as e:
    print(f"âŒ Error loading model: {e}")

"""
FIXED GENDER PREDICTION CODE
The issue was with model output interpretation and image preprocessing
"""

import cv2
import numpy as np
from tensorflow.keras.models import load_model

# ============================================================================
# LOAD MODEL
# ============================================================================
GENDER_MODEL_PATH = "/content/gender_cnn_model.h5"
gender_model = load_model(GENDER_MODEL_PATH)
print("âœ… Gender CNN model loaded.")

# Get model input shape
model_input_shape = gender_model.input_shape
IMG_SIZE = (model_input_shape[1], model_input_shape[2])  # (height, width)
print(f"ðŸ“ Model expects input size: {IMG_SIZE}")

# Check model output
print(f"ðŸ“Š Model output shape: {gender_model.output_shape}")

# ============================================================================
# FACE DETECTION (Keep as is)
# ============================================================================
FACE_CASCADE = cv2.CascadeClassifier(
    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
)

def get_face_from_person(person_bgr):
    """Extract face from person crop using Haar Cascade"""
    if person_bgr is None or person_bgr.size == 0:
        return None

    gray = cv2.cvtColor(person_bgr, cv2.COLOR_BGR2GRAY)
    faces = FACE_CASCADE.detectMultiScale(gray, 1.2, 3, minSize=(32, 32))

    if len(faces) > 0:
        x, y, w, h = max(faces, key=lambda f: f[2] * f[3])  # largest face
        return person_bgr[y:y+h, x:x+w]

    # Fallback: upper center region (likely contains face)
    H, W = person_bgr.shape[:2]
    y1, y2 = 0, max(32, int(0.45 * H))
    x1, x2 = int(0.2 * W), int(0.8 * W)
    face_region = person_bgr[y1:y2, x1:x2]

    return face_region if face_region.size > 0 else person_bgr


# ============================================================================
# FIXED GENDER PREDICTION
# ============================================================================
def predict_gender(person_bgr):
    """
    FIXED VERSION - Correctly predicts gender from person crop
    Using single sigmoid output logic where > 0.5 is Male
    """
    if person_bgr is None or person_bgr.size == 0:
        return "Unknown"

    # Step 1: Get face (or full person if face not detected)
    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        face = person_bgr

    # Step 2: Resize to model input size
    img = cv2.resize(face, IMG_SIZE)

    # Step 3: Convert BGR to RGB (CRITICAL!)
    # OpenCV uses BGR, but most models train on RGB
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Step 4: Normalize (match your training preprocessing!)
    img_normalized = img_rgb.astype("float32") / 255.0

    # Step 5: Add batch dimension
    img_batch = np.expand_dims(img_normalized, axis=0)

    # Step 6: Predict
    pred = gender_model.predict(img_batch, verbose=0)

    # Step 7: Interpret output - Assuming single sigmoid output where > 0.5 is Male
    prob = float(pred[0][0])
    gender = "Male" if prob > 0.5 else "Female"
    # Confidence is the probability of the predicted class
    confidence = prob if gender == "Male" else 1 - prob


    # print(f"Debug: Raw pred={pred[0]}, Gender={gender}, Conf={confidence:.2f}") # Optional debug print

    return gender


# ============================================================================
# ALTERNATIVE: Test your model to understand output format
# ============================================================================
def test_model_output():
    """
    Run this ONCE to understand your model's output format
    """
    print("\n" + "="*60)
    print("TESTING MODEL OUTPUT FORMAT")
    print("="*60)

    # Create dummy image
    dummy_img = np.random.rand(1, IMG_SIZE[0], IMG_SIZE[1], 3).astype('float32')

    # Get prediction
    pred = gender_model.predict(dummy_img, verbose=0)

    print(f"\nðŸ“Š Output shape: {pred.shape}")
    print(f"ðŸ“Š Output values: {pred[0]}")
    print(f"ðŸ“Š Output sum: {np.sum(pred[0]):.4f}")

    if pred.shape[-1] == 1:
        print("\nâœ… Model type: Single sigmoid output")
        print("   - Output range: 0-1")
        print("   - Interpretation: >0.5 typically means positive class")
    elif pred.shape[-1] == 2:
        print("\nâœ… Model type: Two-class softmax output")
        print("   - Output range: Both sum to ~1.0")
        print(f"   - Class 0 (likely Female): {pred[0][0]:.4f}")
        print(f"   - Class 1 (likely Male): {pred[0][1]:.4f}")

    print("\n" + "="*60)

# Uncomment to test:
# test_model_output()


# ============================================================================
# ENHANCED VERSION: Test with actual image
# ============================================================================
def test_with_sample_image(image_path):
    """
    Test prediction on a sample image to verify correctness
    Usage: test_with_sample_image("sample_male.jpg")
    """
    img = cv2.imread(image_path)
    if img is None:
        print(f"âŒ Could not load: {image_path}")
        return

    gender = predict_gender(img)
    print(f"\nðŸ“¸ Image: {image_path}")
    print(f"ðŸ‘¤ Predicted: {gender}")

    # Show prediction on image
    cv2.putText(img, f"Predicted: {gender}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow("Prediction Test", img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


# ============================================================================
# QUICK FIX: If predictions are REVERSED
# ============================================================================
def predict_gender_REVERSED(person_bgr):
    """
    Use this if your predictions are consistently backwards
    (detecting males as females and vice versa)
    """
    if person_bgr is None or person_bgr.size == 0:
        return "Unknown"

    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        face = person_bgr

    img = cv2.resize(face, IMG_SIZE)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_normalized = img_rgb.astype("float32") / 255.0
    img_batch = np.expand_dims(img_normalized, axis=0)

    pred = gender_model.predict(img_batch, verbose=0)

    if pred.shape[-1] == 1:
        prob = float(pred[0][0])
        # REVERSED: flip the interpretation
        gender = "Female" if prob > 0.5 else "Male"  # <-- SWAPPED
        confidence = prob if gender == "Female" else 1 - prob

    elif pred.shape[-1] == 2:
        female_prob = float(pred[0][0])
        male_prob = float(pred[0][1])

        # REVERSED: swap the class assignments
        if male_prob > female_prob:
            gender = "Female"  # <-- SWAPPED
            confidence = male_prob
        else:
            gender = "Male"  # <-- SWAPPED
            confidence = female_prob

    return gender


# ============================================================================
# RECOMMENDED: Add confidence threshold
# ============================================================================
def predict_gender_with_confidence(person_bgr, confidence_threshold=0.7):
    """
    Only return gender if confidence is above threshold
    Otherwise return "Unknown"
    """
    if person_bgr is None or person_bgr.size == 0:
        return "Unknown"

    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        face = person_bgr

    img = cv2.resize(face, IMG_SIZE)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_normalized = img_rgb.astype("float32") / 255.0
    img_batch = np.expand_dims(img_normalized, axis=0)

    pred = gender_model.predict(img_batch, verbose=0)

    if pred.shape[-1] == 1:
        prob = float(pred[0][0])
        gender = "Male" if prob > 0.5 else "Female"
        confidence = prob if gender == "Male" else 1 - prob

    elif pred.shape[-1] == 2:
        female_prob = float(pred[0][0])
        male_prob = float(pred[0][1])

        if male_prob > female_prob:
            gender = "Male"
            confidence = male_prob
        else:
            gender = "Female"
            confidence = female_prob

    # Return Unknown if confidence too low
    if confidence < confidence_threshold:
        return "Unknown"

    return gender


# ============================================================================
# DEBUGGING: Print detailed prediction info
# ============================================================================
def predict_gender_DEBUG(person_bgr):
    """
    Use this version to see what's happening
    """
    if person_bgr is None or person_bgr.size == 0:
        print("âš ï¸  Empty person crop")
        return "Unknown"

    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        print("âš ï¸  No face detected, using full crop")
        face = person_bgr

    print(f"ðŸ“ Face crop size: {face.shape}")

    img = cv2.resize(face, IMG_SIZE)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_normalized = img_rgb.astype("float32") / 255.0
    img_batch = np.expand_dims(img_normalized, axis=0)

    print(f"ðŸ“Š Input to model: shape={img_batch.shape}, min={img_batch.min():.3f}, max={img_batch.max():.3f}")

    pred = gender_model.predict(img_batch, verbose=0)
    print(f"ðŸ“Š Model output: {pred[0]}")

    if pred.shape[-1] == 1:
        prob = float(pred[0][0])
        gender = "Male" if prob > 0.5 else "Female"
        confidence = prob if gender == "Male" else 1 - prob
        print(f"ðŸ‘¤ Single output: prob={prob:.4f}, gender={gender}, confidence={confidence:.4f}")

    elif pred.shape[-1] == 2:
        female_prob = float(pred[0][0])
        male_prob = float(pred[0][1])
        gender = "Male" if male_prob > female_prob else "Female"
        confidence = max(female_prob, male_prob)
        print(f"ðŸ‘¤ Two outputs: female={female_prob:.4f}, male={male_prob:.4f}, final={gender}")

    return gender


print("\nâœ… Fixed gender prediction functions loaded!")
print("\nðŸ“ Usage:")
print("   - Use: predict_gender(person_crop)")
print("   - Debug: predict_gender_DEBUG(person_crop)")
print("   - If reversed: predict_gender_REVERSED(person_crop)")
print("   - Test output: test_model_output()")

import os

GENDER_MODEL_PATH = '/content/gender_cnn_model.h5'

if os.path.exists(GENDER_MODEL_PATH):
    file_size_bytes = os.path.getsize(GENDER_MODEL_PATH)
    print(f"The size of '{GENDER_MODEL_PATH}' is {file_size_bytes} bytes.")
else:
    print(f"The file '{GENDER_MODEL_PATH}' does not exist.")

import os

file_path = "/content/test_video.mp4"

if os.path.exists(file_path):
    print(f"'{file_path}' exists.")
else:
    print(f"'{file_path}' does not exist. Please upload the video file.")

# Commented out IPython magic to ensure Python compatibility.
# %pip install ultralytics

"""Complete Harassment Detection System with Improved Accuracy"""

"""
Complete Harassment Detection System with Improved Accuracy
"""

import os
import time
import math
import uuid
import json
import cv2
import numpy as np
from collections import deque, defaultdict, Counter
from dataclasses import dataclass, field
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime

# Deep Learning imports
from ultralytics import YOLO
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array

"""start

"""

# Re-run imports after installation
import os
import time
import math
import uuid
import json
import cv2
import numpy as np
from collections import deque, defaultdict, Counter
from dataclasses import dataclass, field
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime

# Deep Learning imports
from ultralytics import YOLO
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array

# Commented out IPython magic to ensure Python compatibility.
# %pip install ultralytics mediapipe

# =========================================================
# 1) CONFIGURATION & PATHS
# =========================================================
import os
# Input/Output Paths
VIDEO_PATH = "test_video.mp4"
OUTPUT_BASE = "outputs"
ANNOTATED_VIDEO = os.path.join(OUTPUT_BASE, "annotated.mp4")
FRAMES_DIR = os.path.join(OUTPUT_BASE, "frames")
RISKY_FRAMES_DIR = os.path.join(OUTPUT_BASE, "risky_frames")
DANGEROUS_SCENES_DIR = os.path.join(OUTPUT_BASE, "dangerous_harassment")  # NEW
RISKY_CLIPS_DIR = os.path.join(OUTPUT_BASE, "risky_clips")
LOG_PATH_JSON = os.path.join(OUTPUT_BASE, "events.json")
LOG_PATH_JSONL = os.path.join(OUTPUT_BASE, "events.jsonl")
STATS_PATH = os.path.join(OUTPUT_BASE, "statistics.json")
GRAPHS_DIR = os.path.join(OUTPUT_BASE, "graphs")

# Create directories
for d in [OUTPUT_BASE, FRAMES_DIR, RISKY_FRAMES_DIR, DANGEROUS_SCENES_DIR,
          RISKY_CLIPS_DIR, GRAPHS_DIR]:
    os.makedirs(d, exist_ok=True)

# Parameters
FPS_FALLBACK = 25
SOS_SECONDS = 3
SURR_SECONDS = 10
COUNT_RISK_COOLDOWN_SEC = 5
NEAR_RADIUS_PX = 120
IMG_SIZE = (128, 128)

# Gender confidence thresholds
GENDER_CONFIDENCE_THRESHOLD = 0.55  # Improved threshold
MIN_FACE_SIZE = 40  # Minimum face size for detection

# Harassment detection thresholds
HARASSMENT_CONFIDENCE_THRESHOLD = 0.7  # How confident before marking as dangerous
MIN_HARASSMENT_DURATION = 2  # seconds

# Colors
BOX_FEMALE = (255, 0, 255)  # Magenta
BOX_MALE = (255, 0, 0)      # Blue
BOX_UNKNOWN = (128, 128, 128)  # Gray
TEXT_COLOR = (255, 255, 255)
DANGER_COLOR = (0, 0, 255)  # Red

# Gender labels
LABELS = ["Male", "Female"]

# =========================================================
# 2) LOAD MODELS
# =========================================================

print("ðŸ”§ Loading models...")

# YOLO for person detection
try:
    yolo = YOLO("yolov8n.pt")
    _ = yolo.predict(np.zeros((640, 640, 3), dtype=np.uint8),
                     device='cpu', classes=[0], verbose=False)
    print("âœ… YOLOv8 loaded (CPU mode)")
except Exception as e:
    print(f"âŒ YOLO load failed: {e}")
    # Depending on criticality, you might want to raise the exception or handle it differently
    # raise

# MediaPipe Pose
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(
    static_image_mode=False,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
print("âœ… MediaPipe Pose loaded")

# Haar Cascade for face detection
FACE_CASCADE = cv2.CascadeClassifier(
    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
)
if FACE_CASCADE.empty():
    raise RuntimeError("âŒ Could not load Haar Cascade")
print("âœ… Haar Cascade face detector loaded")

# Gender CNN Model
GENDER_MODEL_PATH = '/content/gender_cnn_model.h5'

# Removed the incorrect FileNotFoundError check
try:
    gender_model = tf.keras.models.load_model(GENDER_MODEL_PATH)
    print(f"âœ… Gender CNN loaded from {GENDER_MODEL_PATH}")
except Exception as e:
    print(f"âŒ Gender model load failed: {e}")
    print("Please ensure the gender_cnn_model.h5 is correctly uploaded and not truncated.")
    raise # Re-raise the exception after printing a user-friendly message

import os

GENDER_MODEL_PATH = '/content/gender_cnn_model.h5'

if os.path.exists(GENDER_MODEL_PATH):
    print(f"âœ… The file '{GENDER_MODEL_PATH}' exists.")
else:
    print(f"âŒ The file '{GENDER_MODEL_PATH}' does not exist.")
    print("Please ensure the gender_cnn_model.h5 is correctly uploaded or saved to this location.")

# =========================================================
# 3) TRACKING DATA STRUCTURES
# =========================================================

@dataclass
class Track:
    tid: int
    bbox: tuple  # (x1, y1, x2, y2)
    lost: int = 0
    sos_frames: int = 0
    gender_hist: deque = field(default_factory=lambda: deque(maxlen=30))
    gender_confidence: deque = field(default_factory=lambda: deque(maxlen=30))
    is_female_pose: bool = False
    last_seen_frame: int = 0
    harassment_score: float = 0.0  # NEW: harassment confidence
    harassment_frames: int = 0      # NEW: consecutive harassment frames

class IOUTracker:
    def __init__(self, iou_thresh=0.35, max_lost=15):
        self.iou_thresh = iou_thresh
        self.max_lost = max_lost
        self.next_id = 0
        self.tracks = {}

    @staticmethod
    def iou(a, b):
        ax1, ay1, ax2, ay2 = a
        bx1, by1, bx2, by2 = b
        inter_x1 = max(ax1, bx1)
        inter_y1 = max(ay1, by1)
        inter_x2 = min(ax2, bx2)
        inter_y2 = min(ay2, by2)
        inter_w = max(0, inter_x2 - inter_x1)
        inter_h = max(0, inter_y2 - inter_y1)
        inter = inter_w * inter_h
        area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)
        area_b = max(0, bx2 - bx1) * max(0, by2 - by1)
        denom = area_a + area_b - inter + 1e-6
        return inter / denom

    def update(self, det_bboxes, frame_idx):
        assigned = set()
        updated = {}

        # Match existing tracks
        for tid, tr in self.tracks.items():
            best_det, best_iou, best_j = None, 0.0, -1
            for j, db in enumerate(det_bboxes):
                if j in assigned:
                    continue
                iouv = self.iou(tr.bbox, db)
                if iouv > best_iou:
                    best_iou, best_det, best_j = iouv, db, j

            if best_det is not None and best_iou >= self.iou_thresh:
                updated[tid] = Track(
                    tid=tid,
                    bbox=best_det,
                    lost=0,
                    sos_frames=tr.sos_frames,
                    gender_hist=tr.gender_hist,
                    gender_confidence=tr.gender_confidence,
                    is_female_pose=tr.is_female_pose,
                    last_seen_frame=frame_idx,
                    harassment_score=tr.harassment_score,
                    harassment_frames=tr.harassment_frames
                )
                assigned.add(best_j)
            else:
                tr.lost += 1
                if tr.lost <= self.max_lost:
                    tr.last_seen_frame = frame_idx
                    updated[tid] = tr

        # Create new tracks
        for j, db in enumerate(det_bboxes):
            if j not in assigned:
                updated[self.next_id] = Track(
                    tid=self.next_id,
                    bbox=db,
                    lost=0,
                    last_seen_frame=frame_idx
                )
                self.next_id += 1

        # Remove lost tracks
        self.tracks = {tid: tr for tid, tr in updated.items()
                      if tr.lost <= self.max_lost}
        return self.tracks

# =========================================================
# 4) HELPER FUNCTIONS
# =========================================================

def wrists_above_shoulders(crop_bgr):
    """Check if both wrists are above shoulders (SOS gesture)"""
    if crop_bgr is None or crop_bgr.size == 0:
        return False

    rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
    res = pose.process(rgb)

    if not res.pose_landmarks:
        return False

    lm = res.pose_landmarks.landmark
    h = crop_bgr.shape[0]

    try:
        lw_y = lm[mp_pose.PoseLandmark.LEFT_WRIST].y * h
        rw_y = lm[mp_pose.PoseLandmark.RIGHT_WRIST].y * h
        ls_y = lm[mp_pose.PoseLandmark.LEFT_SHOULDER].y * h
        rs_y = lm[mp_pose.PoseLandmark.RIGHT_SHOULDER].y * h
        return (lw_y < ls_y) and (rw_y < rs_y)
    except:
        return False

def get_face_from_person(person_bgr):
    """Extract face region using Haar Cascade"""
    if person_bgr is None or person_bgr.size == 0:
        return None

    gray = cv2.cvtColor(person_bgr, cv2.COLOR_BGR2GRAY)
    faces = FACE_CASCADE.detectMultiScale(
        gray, 1.2, 3, minSize=(MIN_FACE_SIZE, MIN_FACE_SIZE)
    )

    if len(faces) > 0:
        x, y, w, h = max(faces, key=lambda f: f[2] * f[3])
        return person_bgr[y:y+h, x:x+w]

    # Fallback: upper center region
    H, W = person_bgr.shape[:2]
    y1, y2 = 0, max(MIN_FACE_SIZE, int(0.4 * H))
    x1, x2 = int(0.25 * W), int(0.75 * W)
    fallback = person_bgr[y1:y2, x1:x2]

    if fallback.size == 0:
        return person_bgr
    return fallback

def predict_gender(person_bgr):
    """
    Improved gender prediction with confidence scoring
    Returns: (gender_str, confidence_score)
    """
    if person_bgr is None or person_bgr.size == 0:
        return "Unknown", 0.0

    # Try to get face first
    face = get_face_from_person(person_bgr)
    if face is None or face.size == 0:
        face = person_bgr

    try:
        # Preprocess
        img = cv2.resize(face, IMG_SIZE)
        img = img_to_array(img)
        img = img.astype("float32") / 255.0
        img = np.expand_dims(img, axis=0)

        # Predict
        pred = gender_model.predict(img, verbose=0)

        # Handle different output formats
        if pred.shape[-1] == 1:
            # Binary sigmoid output
            conf = float(pred[0][0])
            gender = "Female" if conf >= 0.5 else "Male"
            confidence = conf if conf >= 0.5 else (1.0 - conf)
        elif pred.shape[-1] == 2:
            # Two-class softmax
            idx = int(np.argmax(pred[0]))
            confidence = float(pred[0][idx])
            gender = LABELS[idx] if idx < len(LABELS) else "Unknown"
        else:
            idx = int(np.argmax(pred[0]))
            confidence = float(pred[0][idx])
            gender = LABELS[idx] if idx < len(LABELS) else "Unknown"

        # Apply confidence threshold
        if confidence < GENDER_CONFIDENCE_THRESHOLD:
            return "Unknown", confidence

        return gender, confidence

    except Exception as e:
        print(f"âš ï¸ Gender prediction error: {e}")
        return "Unknown", 0.0

def center_of(bbox):
    x1, y1, x2, y2 = bbox
    return (0.5 * (x1 + x2), 0.5 * (y1 + y2))

def proximity_counts(female_tracks, male_tracks, radius_px=NEAR_RADIUS_PX):
    """Count nearby males for each female"""
    counts = {}
    male_centers = {m.tid: center_of(m.bbox) for m in male_tracks}

    for f in female_tracks:
        fx, fy = center_of(f.bbox)
        cnt = 0
        for _, (mx, my) in male_centers.items():
            if math.hypot(mx - fx, my - fy) <= radius_px:
                cnt += 1
        counts[f.tid] = cnt

    return counts

def calculate_harassment_score(female_track, male_tracks, frame_context):
    """
    Calculate harassment likelihood score based on multiple factors
    Returns: score (0.0 to 1.0)
    """
    if not male_tracks:
        return 0.0

    score = 0.0
    fx, fy = center_of(female_track.bbox)

    # Factor 1: Number of nearby males (0-0.3)
    nearby_males = 0
    for m in male_tracks:
        mx, my = center_of(m.bbox)
        dist = math.hypot(mx - fx, my - fy)
        if dist <= NEAR_RADIUS_PX:
            nearby_males += 1

    if nearby_males >= 4:
        score += 0.3
    elif nearby_males >= 2:
        score += 0.15

    # Factor 2: SOS gesture (0-0.4)
    if female_track.sos_frames > 0:
        score += 0.4

    # Factor 3: Surrounding pattern (0-0.3)
    # Check if males are surrounding (in different directions)
    if nearby_males >= 2:
        angles = []
        for m in male_tracks:
            mx, my = center_of(m.bbox)
            dist = math.hypot(mx - fx, my - fy)
            if dist <= NEAR_RADIUS_PX:
                angle = math.atan2(my - fy, mx - fx)
                angles.append(angle)

        # Check angle distribution (good surrounding = diverse angles)
        if len(angles) >= 2:
            angles = sorted(angles)
            max_gap = max(angles[i+1] - angles[i]
                         for i in range(len(angles)-1))
            if max_gap < math.pi:  # Males are well distributed
                score += 0.3

    return min(score, 1.0)

def draw_box_label(img, bbox, label, color, thickness=2):
    """Draw bounding box with label"""
    x1, y1, x2, y2 = map(int, bbox)
    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)

    # Background for text
    (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
    cv2.rectangle(img, (x1, y1 - th - 4), (x1 + tw, y1), color, -1)
    cv2.putText(img, label, (x1, y1 - 4),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, TEXT_COLOR, 2)

def save_clip_from_buffer(buffer_deque, clip_path, fps, size):
    """Save video clip from frame buffer"""
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    vw = cv2.VideoWriter(clip_path, fourcc, fps, size)
    for f in buffer_deque:
        vw.write(f)
    vw.release()

# =========================================================
# 5) MAIN VIDEO PROCESSING LOOP
# =========================================================

print(f"\nðŸŽ¬ Opening video: {VIDEO_PATH}")
cap = cv2.VideoCapture(VIDEO_PATH)
if not cap.isOpened():
    raise RuntimeError(f"âŒ Could not open: {VIDEO_PATH}")

fps = cap.get(cv2.CAP_PROP_FPS)
fps = int(fps) if fps and fps > 1 else FPS_FALLBACK
W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
N = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

print(f"ðŸ“Š Video info: {N} frames, {fps} FPS, {W}x{H}")

# Video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
writer = cv2.VideoWriter(ANNOTATED_VIDEO, fourcc, fps, (W, H))

# Buffers and trackers
frame_buffer = deque(maxlen=4 * fps)
tracker = IOUTracker(iou_thresh=0.35, max_lost=15)

# State tracking
last_count_risk_ts = 0.0
events = []
surr_hold = defaultdict(float)
sos_fps_needed = int(SOS_SECONDS * fps)
surr_fps_needed = int(SURR_SECONDS * fps)
harassment_fps_needed = int(MIN_HARASSMENT_DURATION * fps)

# Statistics
stats = {
    "total_frames": N,
    "fps": fps,
    "female_detections": 0,
    "male_detections": 0,
    "unknown_detections": 0,
    "sos_events": 0,
    "surrounded_events": 0,
    "count_risk_events": 0,
    "harassment_events": 0,
    "gender_timeline": [],
    "event_timeline": []
}

# Process frames
frame_idx = 0
pbar = tqdm(total=N, desc="Processing video")

while True:
    ok, frame = cap.read()
    if not ok:
        break

    frame_idx += 1
    now_ts = time.time()
    frame_buffer.append(frame.copy())

    # ========== DETECTION ==========
    yres = yolo.predict(source=frame, classes=[0], device='cpu', verbose=False)[0]
    dets = []
    for xyxy in yres.boxes.xyxy.cpu().numpy():
        x1, y1, x2, y2 = map(int, xyxy[:4])
        if x2 > x1 and y2 > y1:
            dets.append((x1, y1, x2, y2))

    # ========== TRACKING ==========
    tracks = tracker.update(dets, frame_idx)

    # ========== GENDER & POSE ANALYSIS ==========
    genders_per_track = {}
    female_tracks, male_tracks = [], []

    for tid, tr in tracks.items():
        x1, y1, x2, y2 = map(int, tr.bbox)
        person = frame[max(0, y1):min(H, y2), max(0, x1):min(W, x2)]

        if person.size == 0:
            continue

        # SOS gesture detection
        sos_now = wrists_above_shoulders(person)
        tr.sos_frames = (tr.sos_frames + 1) if sos_now else 0

        # Gender prediction
        gender, confidence = predict_gender(person)

        # Override with SOS gesture (strong female indicator)
        if sos_now and confidence < 0.8:
            gender = "Female"
            confidence = 0.9

        tr.gender_hist.append(gender)
        tr.gender_confidence.append(confidence)

        # Majority vote with confidence weighting
        if len(tr.gender_hist) >= 5:
            gender_votes = list(tr.gender_hist)
            confidence_votes = list(tr.gender_confidence)

            # Weighted voting
            female_score = sum(c for g, c in zip(gender_votes, confidence_votes)
                             if g == "Female")
            male_score = sum(c for g, c in zip(gender_votes, confidence_votes)
                           if g == "Male")

            if female_score > male_score and female_score > 0.5:
                majority = "Female"
            elif male_score > female_score and male_score > 0.5:
                majority = "Male"
            else:
                majority = "Unknown"
        else:
            majority = gender

        genders_per_track[tid] = majority

        # Categorize tracks
        if majority == "Female":
            female_tracks.append(tr)
            stats["female_detections"] += 1
        elif majority == "Male":
            male_tracks.append(tr)
            stats["male_detections"] += 1
        else:
            stats["unknown_detections"] += 1

        # Annotate
        conf_str = f"{confidence:.0%}" if confidence > 0 else ""
        label = f"ID{tid}:{majority} {conf_str}"
        if tr.sos_frames >= sos_fps_needed:
            label += " [SOS]"

        box_color = BOX_FEMALE if majority == "Female" else (
            BOX_MALE if majority == "Male" else BOX_UNKNOWN
        )
        draw_box_label(frame, tr.bbox, label, box_color)

    # Record gender timeline
    stats["gender_timeline"].append({
        "frame": frame_idx,
        "females": len(female_tracks),
        "males": len(male_tracks)
    })

    # ========== RISK DETECTION ==========
    females_count = len(female_tracks)
    males_count = len(male_tracks)

    # (A) COUNT RISK
    count_risk = (females_count >= 1 and males_count > females_count)

    # (B) SOS GESTURE EVENTS
    for tid, tr in tracks.items():
        if tr.sos_frames >= sos_fps_needed:
            ev_id = uuid.uuid4().hex
            screenshot = os.path.join(RISKY_FRAMES_DIR, f"sos_{ev_id}.jpg")
            cv2.imwrite(screenshot, frame)

            clip_path = os.path.join(RISKY_CLIPS_DIR, f"sos_{ev_id}.mp4")
            save_clip_from_buffer(frame_buffer, clip_path, fps, (W, H))

            events.append({
                "id": ev_id,
                "type": "SOS_GESTURE",
                "frame": frame_idx,
                "ts": now_ts,
                "track_id": tid,
                "female_count": females_count,
                "male_count": males_count,
                "screenshot": screenshot,
                "clip": clip_path
            })
            stats["sos_events"] += 1
            stats["event_timeline"].append({"frame": frame_idx, "type": "SOS"})
            tr.sos_frames = 0

    # (C) HARASSMENT DETECTION (NEW)
    harassment_detected = False
    for ft in female_tracks:
        h_score = calculate_harassment_score(ft, male_tracks, frame)
        ft.harassment_score = h_score

        if h_score >= HARASSMENT_CONFIDENCE_THRESHOLD:
            ft.harassment_frames += 1
        else:
            ft.harassment_frames = 0

        # Trigger harassment event
        if ft.harassment_frames >= harassment_fps_needed:
            ev_id = uuid.uuid4().hex

            # Save to dangerous scenes folder
            danger_screenshot = os.path.join(
                DANGEROUS_SCENES_DIR,
                f"harassment_{ev_id}_{datetime.now().strftime('%H%M%S')}.jpg"
            )
            cv2.imwrite(danger_screenshot, frame)

            # Also save to risky frames
            screenshot = os.path.join(RISKY_FRAMES_DIR, f"harassment_{ev_id}.jpg")
            cv2.imwrite(screenshot, frame)

            clip_path = os.path.join(RISKY_CLIPS_DIR, f"harassment_{ev_id}.mp4")
            save_clip_from_buffer(frame_buffer, clip_path, fps, (W, H))

            nearby_males = proximity_counts([ft], male_tracks).get(ft.tid, 0)

            events.append({
                "id": ev_id,
                "type": "HARASSMENT_DETECTED",
                "frame": frame_idx,
                "ts": now_ts,
                "female_track_id": ft.tid,
                "harassment_score": float(h_score),
                "nearby_males": nearby_males,
                "screenshot": screenshot,
                "danger_screenshot": danger_screenshot,
                "clip": clip_path,
                "description": "High confidence harassment scene detected"
            })

            stats["harassment_events"] += 1
            stats["event_timeline"].append({"frame": frame_idx, "type": "HARASSMENT"})
            harassment_detected = True
            ft.harassment_frames = 0

            # Draw danger warning
            cv2.putText(frame, "âš ï¸ HARASSMENT DETECTED!", (30, 80),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, DANGER_COLOR, 3)

    # (D) SURROUNDED DETECTION
    if female_tracks and male_tracks:
        near = proximity_counts(female_tracks, male_tracks, radius_px=NEAR_RADIUS_PX)
        for ft in female_tracks:
            cnt = near.get(ft.tid, 0)
            if cnt >= 4:
                surr_hold[ft.tid] += 1
            else:
                surr_hold[ft.tid] = 0

            if surr_hold[ft.tid] >= surr_fps_needed:
                ev_id = uuid.uuid4().hex
                screenshot = os.path.join(RISKY_FRAMES_DIR, f"surrounded_{ev_id}.jpg")
                cv2.imwrite(screenshot, frame)

                clip_path = os.path.join(RISKY_CLIPS_DIR, f"surrounded_{ev_id}.mp4")
                save_clip_from_buffer(frame_buffer, clip_path, fps, (W, H))

                events.append({
                    "id": ev_id,
                    "type": "FEMALE_SURROUNDED",
                    "frame": frame_idx,
                    "ts": now_ts,
                    "female_track_id": ft.tid,
                    "near_males": cnt,
                    "screenshot": screenshot,
                    "clip": clip_path
                })
                stats["surrounded_events"] += 1
                stats["event_timeline"].append({"frame": frame_idx, "type": "SURROUNDED"})
                surr_hold[ft.tid] = 0

    # (E) COUNT RISK EVENTS
    if count_risk:
        if now_ts - last_count_risk_ts >= COUNT_RISK_COOLDOWN_SEC:
            ev_id = uuid.uuid4().hex
            screenshot = os.path.join(RISKY_FRAMES_DIR, f"count_{ev_id}.jpg")
            cv2.imwrite(screenshot, frame)

            clip_path = os.path.join(RISKY_CLIPS_DIR, f"count_{ev_id}.mp4")
            save_clip_from_buffer(frame_buffer, clip_path, fps, (W, H))

            events.append({
                "id": ev_id,
                "type": "COUNT_RISK",
                "frame": frame_idx,
                "ts": now_ts,
                "female_count": females_count,
                "male_count": males_count,
                "screenshot": screenshot,
                "clip": clip_path
            })
            stats["count_risk_events"] += 1
            stats["event_timeline"].append({"frame": frame_idx, "type": "COUNT_RISK"})
            last_count_risk_ts = now_ts

        cv2.putText(frame, f"âš ï¸ RISK: {males_count} males > {females_count} females",
                   (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, DANGER_COLOR, 2)

    # Write annotated frame
    writer.write(frame)

    # Save sample frames
    if frame_idx % max(1, fps) == 0:
        cv2.imwrite(os.path.join(FRAMES_DIR, f"frame_{frame_idx:06d}.jpg"), frame)

    pbar.update(1)

pbar.close()
cap.release()
writer.release()



# =========================================================
# 6) SAVE LOGS & GENERATE REPORTS
# =========================================================

print("\nðŸ“ Saving logs...")

# Save events
with open(LOG_PATH_JSON, "w") as f:
    json.dump(events, f, indent=2)

with open(LOG_PATH_JSONL, "w") as f:
    for e in events:
        f.write(json.dumps(e) + "\n")

# Save statistics
with open(STATS_PATH, "w") as f:
    json.dump(stats, f, indent=2)

# =========================================================
# 7) GENERATE GRAPHS & VISUALIZATIONS
# =========================================================

print("\nðŸ“Š Generating graphs...")

# Graph 1: Gender detection over time
plt.figure(figsize=(14, 6))
frames = [t["frame"] for t in stats["gender_timeline"]]
females = [t["females"] for t in stats["gender_timeline"]]
males = [t["males"] for t in stats["gender_timeline"]]

plt.plot(frames, females, label="Females", color='magenta', linewidth=2)
plt.plot(frames, males, label="Males", color='blue', linewidth=2)
plt.xlabel("Frame Number", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.title("Gender Detection Over Time", fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(GRAPHS_DIR, "gender_timeline.png"), dpi=300)
plt.close()

# Graph 2: Event distribution (pie chart)
event_types = {
    "SOS Gestures": stats["sos_events"],
    "Harassment": stats["harassment_events"],
    "Surrounded": stats["surrounded_events"],
    "Count Risk": stats["count_risk_events"]
}
event_types = {k: v for k, v in event_types.items() if v > 0}

if event_types:
    plt.figure(figsize=(10, 8))
    colors = ['#FF6B6B', '#FF0000', '#FFA500', '#FFD700']
    explode = [0.1 if v == max(event_types.values()) else 0 for v in event_types.values()]

    wedges, texts, autotexts = plt.pie(
        event_types.values(),
        labels=event_types.keys(),
        autopct='%1.1f%%',
        colors=colors,
        explode=explode,
        shadow=True,
        startangle=90
    )

    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontsize(12)
        autotext.set_fontweight('bold')

    for text in texts:
        text.set_fontsize(12)
        text.set_fontweight('bold')

    plt.title("Distribution of Risk Events", fontsize=14, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig(os.path.join(GRAPHS_DIR, "event_distribution.png"), dpi=300)
    plt.close()

# Graph 3: Event timeline
if stats["event_timeline"]:
    event_frames = [e["frame"] for e in stats["event_timeline"]]
    event_types_list = [e["type"] for e in stats["event_timeline"]]

    plt.figure(figsize=(14, 6))

    # Color mapping
    color_map = {
        "SOS": "red",
        "HARASSMENT": "darkred",
        "SURROUNDED": "orange",
        "COUNT_RISK": "gold"
    }

    colors_list = [color_map.get(et, "gray") for et in event_types_list]

    plt.scatter(event_frames, range(len(event_frames)),
               c=colors_list, s=100, alpha=0.7, edgecolors='black', linewidth=1.5)

    plt.xlabel("Frame Number", fontsize=12)
    plt.ylabel("Event Index", fontsize=12)
    plt.title("Risk Events Timeline", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)

    # Create legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor=color, label=label)
                      for label, color in color_map.items()]
    plt.legend(handles=legend_elements, loc='upper right', fontsize=10)

    plt.tight_layout()
    plt.savefig(os.path.join(GRAPHS_DIR, "event_timeline.png"), dpi=300)
    plt.close()

# Graph 4: Detection accuracy bar chart
plt.figure(figsize=(10, 6))
categories = ['Female', 'Male', 'Unknown']
counts = [
    stats["female_detections"],
    stats["male_detections"],
    stats["unknown_detections"]
]
colors_bar = ['magenta', 'blue', 'gray']

bars = plt.bar(categories, counts, color=colors_bar, alpha=0.7, edgecolor='black', linewidth=2)

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{int(height)}',
            ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.xlabel("Gender Category", fontsize=12)
plt.ylabel("Total Detections", fontsize=12)
plt.title("Gender Detection Summary", fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig(os.path.join(GRAPHS_DIR, "detection_summary.png"), dpi=300)
plt.close()

# Graph 5: Risk level over time (heatmap style)
if len(frames) > 0:
    # Calculate risk score for each frame
    risk_scores = []
    for i in range(len(frames)):
        f_count = females[i]
        m_count = males[i]

        # Simple risk calculation
        risk = 0
        if f_count > 0:
            if m_count > f_count * 2:
                risk = 3  # High risk
            elif m_count > f_count:
                risk = 2  # Medium risk
            elif f_count >= m_count:
                risk = 1  # Low risk

        risk_scores.append(risk)

    plt.figure(figsize=(14, 4))

    # Create color-coded timeline
    colors_risk = ['green' if r == 1 else 'yellow' if r == 2 else 'red' if r == 3 else 'white'
                   for r in risk_scores]

    plt.scatter(frames, [1]*len(frames), c=colors_risk, s=5, marker='|', alpha=0.8)
    plt.ylim(0.5, 1.5)
    plt.xlabel("Frame Number", fontsize=12)
    plt.title("Risk Level Timeline", fontsize=14, fontweight='bold')
    plt.yticks([])
    plt.grid(True, alpha=0.3, axis='x')

    # Legend
    from matplotlib.patches import Patch
    legend_risk = [
        Patch(facecolor='green', label='Low Risk'),
        Patch(facecolor='yellow', label='Medium Risk'),
        Patch(facecolor='red', label='High Risk')
    ]
    plt.legend(handles=legend_risk, loc='upper right', fontsize=10)

    plt.tight_layout()
    plt.savefig(os.path.join(GRAPHS_DIR, "risk_timeline.png"), dpi=300)
    plt.close()

# =========================================================
# 8) GENERATE TEXT REPORT
# =========================================================

report_path = os.path.join(OUTPUT_BASE, "analysis_report.txt")
with open(report_path, "w") as report:
    report.write("=" * 70 + "\n")
    report.write("HARASSMENT DETECTION SYSTEM - ANALYSIS REPORT\n")
    report.write("=" * 70 + "\n\n")

    report.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report.write(f"Video: {VIDEO_PATH}\n")
    report.write(f"Total Frames: {stats['total_frames']}\n")
    report.write(f"FPS: {stats['fps']}\n")
    report.write(f"Duration: {stats['total_frames'] / stats['fps']:.2f} seconds\n\n")

    report.write("-" * 70 + "\n")
    report.write("DETECTION SUMMARY\n")
    report.write("-" * 70 + "\n")
    report.write(f"Female Detections: {stats['female_detections']}\n")
    report.write(f"Male Detections: {stats['male_detections']}\n")
    report.write(f"Unknown Detections: {stats['unknown_detections']}\n")

    total_detections = (stats['female_detections'] + stats['male_detections'] +
                       stats['unknown_detections'])
    if total_detections > 0:
        report.write(f"\nAccuracy Metrics:\n")
        report.write(f"  Female Recognition: {stats['female_detections']/total_detections*100:.1f}%\n")
        report.write(f"  Male Recognition: {stats['male_detections']/total_detections*100:.1f}%\n")
        report.write(f"  Unknown Rate: {stats['unknown_detections']/total_detections*100:.1f}%\n")

    report.write("\n" + "-" * 70 + "\n")
    report.write("RISK EVENTS DETECTED\n")
    report.write("-" * 70 + "\n")
    report.write(f"ðŸš¨ HARASSMENT Events: {stats['harassment_events']}\n")
    report.write(f"ðŸ†˜ SOS Gestures: {stats['sos_events']}\n")
    report.write(f"âš ï¸  Surrounded Events: {stats['surrounded_events']}\n")
    report.write(f"âš ï¸  Count Risk Events: {stats['count_risk_events']}\n")
    report.write(f"\nTotal Risk Events: {len(events)}\n")

    if stats['harassment_events'] > 0:
        report.write(f"\nâš ï¸  CRITICAL: {stats['harassment_events']} harassment scene(s) detected!\n")
        report.write(f"   Check folder: {DANGEROUS_SCENES_DIR}\n")

    report.write("\n" + "-" * 70 + "\n")
    report.write("DETAILED EVENT LOG\n")
    report.write("-" * 70 + "\n")

    for i, event in enumerate(events, 1):
        report.write(f"\nEvent #{i}:\n")
        report.write(f"  Type: {event['type']}\n")
        report.write(f"  Frame: {event['frame']}\n")
        report.write(f"  Time: {event['frame'] / stats['fps']:.2f}s\n")

        if event['type'] == 'HARASSMENT_DETECTED':
            report.write(f"  âš ï¸  Harassment Score: {event.get('harassment_score', 0):.2%}\n")
            report.write(f"  Nearby Males: {event.get('nearby_males', 0)}\n")
        elif event['type'] == 'FEMALE_SURROUNDED':
            report.write(f"  Nearby Males: {event.get('near_males', 0)}\n")
        elif event['type'] == 'COUNT_RISK':
            report.write(f"  Female Count: {event.get('female_count', 0)}\n")
            report.write(f"  Male Count: {event.get('male_count', 0)}\n")

        report.write(f"  Screenshot: {event['screenshot']}\n")
        if 'danger_screenshot' in event:
            report.write(f"  âš ï¸  Danger Screenshot: {event['danger_screenshot']}\n")

    report.write("\n" + "=" * 70 + "\n")
    report.write("END OF REPORT\n")
    report.write("=" * 70 + "\n")

# =========================================================
# 9) FINAL OUTPUT SUMMARY
# =========================================================

print("\n" + "=" * 70)
print("âœ… PROCESSING COMPLETE!")
print("=" * 70)
print(f"\nðŸ“¹ Annotated Video: {ANNOTATED_VIDEO}")
print(f"ðŸ“¸ Risky Screenshots: {RISKY_FRAMES_DIR} ({len(events)} events)")
print(f"ðŸš¨ Dangerous Scenes: {DANGEROUS_SCENES_DIR} ({stats['harassment_events']} scenes)")
print(f"ðŸŽ¬ Risky Clips (4s): {RISKY_CLIPS_DIR}")
print(f"ðŸ“ Event Logs: {LOG_PATH_JSON} and {LOG_PATH_JSONL}")
print(f"ðŸ“Š Statistics: {STATS_PATH}")
print(f"ðŸ“ˆ Graphs: {GRAPHS_DIR}")
print(f"ðŸ“„ Text Report: {report_path}")

print(f"\nðŸ“Š DETECTION SUMMARY:")
print(f"   â€¢ Female detections: {stats['female_detections']}")
print(f"   â€¢ Male detections: {stats['male_detections']}")
print(f"   â€¢ Unknown detections: {stats['unknown_detections']}")

print(f"\nðŸš¨ RISK EVENTS:")
print(f"   â€¢ Harassment detected: {stats['harassment_events']}")
print(f"   â€¢ SOS gestures: {stats['sos_events']}")
print(f"   â€¢ Surrounded events: {stats['surrounded_events']}")
print(f"   â€¢ Count risk events: {stats['count_risk_events']}")
print(f"   â€¢ Total events: {len(events)}")

if stats['harassment_events'] > 0:
    print(f"\nâš ï¸  WARNING: {stats['harassment_events']} HARASSMENT SCENE(S) DETECTED!")
    print(f"   Check dangerous scenes folder: {DANGEROUS_SCENES_DIR}")

print("\n" + "=" * 70)
print("ðŸŽ‰ All outputs saved successfully!")
print("=" * 70 + "\n")